{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Multi-step workflow executed by an ai agent to solve complex problems made using IBM Watsonx, beeai and DuckDuckGo search\n",
    "## Create a concise 1-page market report for {company_name} that summarizes the research, competitive analysis, and highlights the market opportunities.\n",
    "\n",
    "#### BeeAI Workflows\n",
    "The agent's behavior is defined through workflow steps and the transitions between them. You can think of a Workflow as a graph that outlines the agent's behavior.\n",
    "#### Basics of Workflows\n",
    "\n",
    "The main components of a BeeAI workflow are state, defined as a Pydantic model, and steps, which are Python functions.\n",
    "\n",
    "- State: Think of state as structured memory that the workflow can read from and write to during execution. It holds the data that flows through the workflow.\n",
    "- Steps: These are the functional components of the workflow, connecting together to perform the agent’s actions.\n",
    "#### A Multi-Step Workflow with Tools\n",
    "\n",
    "Now that you understand the basic components of a Workflow, let’s explore the power of BeeAI Workflows by building a simple web search agent.\n",
    "\n",
    "This agent creates a search query based on an input question, runs the query to retrieve search results, and then generates an answer to the question based on the results.\n",
    "\n",
    "Let’s begin by importing the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all needed imports\n",
    "from pydantic import Field\n",
    "from pydantic import BaseModel, ValidationError\n",
    "from beeai_framework.workflows import Workflow, WorkflowError\n",
    "from beeai_framework.backend import ChatModel, ChatModelOutput, ChatModelStructureOutput, UserMessage\n",
    "from beeai_framework.template import PromptTemplate, PromptTemplateInput\n",
    "from beeai import Bee # Tool  # BeeHive,\n",
    "from beeai_framework.tools import Tool\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "import json\n",
    "from typing import Any\n",
    "from beeai_framework.agents.react import ReActAgent, ReActAgentRunOutput\n",
    "from beeai_framework.backend import ChatModel\n",
    "from beeai_framework.adapters.watsonx import WatsonxChatModel\n",
    "from beeai_framework.emitter import Emitter, EmitterOptions, EventMeta\n",
    "from beeai_framework.memory import UnconstrainedMemory\n",
    "from beeai_framework.backend import ChatModel, ChatModelOutput, UserMessage\n",
    "from beeai_framework.adapters.watsonx import WatsonxChatModel\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can define our workflow State.\n",
    "\n",
    "In this case, the question field is required when instantiating the State. The other fields, search_results and answer, are optional during construction (defaulting to None), but they will be populated by the workflow steps as the execution progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow State\n",
    "class SearchAgentState(BaseModel):\n",
    "    question: str\n",
    "    search_results: str | None = None\n",
    "    answer: str | None = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the ChatModel instance that will handle interaction with our LLM. For this example, we'll use IBM Granite 3.1 8B via Ollama. This model will be used to process the search query and generate answers based on the retrieved results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "# Required\n",
    "WATSONX_URL=os.getenv(\"WATSONX_URL\")\n",
    "WATSONX_API_URL = os.getenv(\"WATSONX_URL\")\n",
    "WATSONX_API_KEY=os.getenv(\"WATSONX_API_KEY\")\n",
    "WATSONX_APIKEY=os.getenv(\"WATSONX_API_KEY\")\n",
    "WX_API_KEY = os.getenv(\"WATSONX_API_KEY\")\n",
    "WATSONX_PROJECT_ID=os.getenv(\"PROJECT_ID\")\n",
    "\n",
    "# Create a ChatModel to interface with ibm/granite-3-8b-instruct from watsonx\n",
    "model = ChatModel.from_name(\n",
    "    \"watsonx:ibm/granite-3-8b-instruct\",\n",
    "    options={\n",
    "        \"project_id\": WATSONX_PROJECT_ID,\n",
    "        \"api_key\": WATSONX_API_KEY,\n",
    "        \"api_base\": WATSONX_API_URL,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a web search agent, we need a way to run web searches. For that, we'll use the DuckDuckGo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web search tool\n",
    "from beeai_framework.tools.search.duckduckgo import DuckDuckGoSearchTool\n",
    "search_tool=[DuckDuckGoSearchTool()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workflow, we make extensive use of PromptTemplates and structured outputs.\n",
    "\n",
    "Here, we define the various templates, input schemas, and structured output schemas that are essential for implementing the agent. These templates will allow us to generate the search query and structure the results in a way that the agent can process effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PromptTemplate Input Schemas\n",
    "class QuestionInput(BaseModel):\n",
    "    question: str\n",
    "\n",
    "\n",
    "class SearchRAGInput(BaseModel):\n",
    "    question: str\n",
    "    search_results: str\n",
    "\n",
    "\n",
    "# Prompt Templates\n",
    "search_query_template = PromptTemplate(\n",
    "    PromptTemplateInput(\n",
    "        schema=QuestionInput,\n",
    "        template=\"\"\"Convert the following question into a concise, effective web search query using keywords and operators for accuracy.\n",
    "Question: {{question}}\"\"\",\n",
    "    )\n",
    ")\n",
    "\n",
    "search_rag_template = PromptTemplate(\n",
    "    PromptTemplateInput(\n",
    "        schema=SearchRAGInput,\n",
    "        template=\"\"\"Search results:\n",
    "{{search_results}}\n",
    "\n",
    "Question: {{question}}\n",
    "Provide a concise answer based on the search results provided. If the results are irrelevant or insufficient, say 'I don't know.' Avoid phrases such as 'According to the results...'.\"\"\",\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Structured output Schemas\n",
    "class WebSearchQuery(BaseModel):\n",
    "    query: str = Field(description=\"The web search query.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can define the first step of the workflow, named web_search.\n",
    "\n",
    "In this step:\n",
    "\n",
    "- The LLM is prompted to generate an effective search query using the search_query_template.\n",
    "- The generated search query is then used to run a web search via the search tool (Duckduckgo).\n",
    "- The search results are stored in the search_results field of the workflow state.\n",
    "- Finally, the step returns generate_answer, passing control to the next step, named generate_answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def web_search(state: SearchAgentState) -> str:\n",
    "    print(\"Step: \", \"web_search\")\n",
    "    # Generate a search query\n",
    "    prompt = search_query_template.render(QuestionInput(question=state.question))\n",
    "    response: ChatModelStructureOutput = await model.create_structure(\n",
    "        schema=WebSearchQuery, messages=[UserMessage(prompt)]\n",
    "    )\n",
    "\n",
    "    # Run search and store results in state\n",
    "    try:\n",
    "        #state.search_results = str(search_tool.run(response.object[\"query\"]))\n",
    "        duckduckgo_tool = DuckDuckGoSearchRun()\n",
    "        state.search_results = duckduckgo_tool.invoke(response.object[\"query\"])\n",
    "    except Exception:\n",
    "        print(\"Search tool failed! Agent will answer from memory.\")\n",
    "        state.search_results = \"No search results available.\"\n",
    "\n",
    "    return \"generate_answer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in the workflow is generate_answer.\n",
    "\n",
    "This step:\n",
    "\n",
    "- Takes the question and search_results from the workflow state.\n",
    "- Uses the search_rag_template to generate an answer based on the provided data.\n",
    "- The generated answer is stored in the workflow state.\n",
    "- Finally, the workflow ends by returning Workflow.END, signaling the completion of the agent’s task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_answer(state: SearchAgentState) -> str:\n",
    "    print(\"Step: \", \"generate_answer\")\n",
    "    # Generate answer based on question and search results from previous step.\n",
    "    prompt = search_rag_template.render(\n",
    "        SearchRAGInput(question=state.question, search_results=state.search_results or \"No results available.\")\n",
    "    )\n",
    "    output: ChatModelOutput = await model.create(messages=[UserMessage(prompt)])\n",
    "\n",
    "    # Store answer in state\n",
    "    state.answer = output.get_text_content()\n",
    "    return Workflow.END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the overall workflow and add the steps we developed earlier. This combines everything into a cohesive agent that can perform web searches and generate answers.\n",
    "\n",
    "## Specify the name of the company for which you want the market report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  web_search\n",
      "Step:  generate_answer\n",
      "*****\n",
      "Question:  Create a concise 1-page market report for company HDFC Bankthat summarizes the research, competitive analysis, and highlights the market opportunities.\n",
      "Answer:  **Market Report: HDFC Bank**\n",
      "\n",
      "**Executive Summary:**\n",
      "\n",
      "HDFC Bank, a leading player in the Indian banking sector, reported a 2% miss in PAT, reaching Rs167bn (1.8% RoA) in the latest quarter. The decline was primarily due to slower credit growth and higher provisions. Despite this, deposit growth remained robust at 16% YoY/2.5% QoQ, bolstered by a QoQ increase of Rs0.6trn. Credit growth, however, dipped to a new low of 3% YoY, reflecting the bank's focus on Loan-to-Deposit Ratio (LDR) management.\n",
      "\n",
      "**Competitive Analysis:**\n",
      "\n",
      "HDFC Bank's competitive strategies and market positioning are noteworthy. The bank continually adapts to evolving customer demands and leverages technological capabilities, ensuring future growth and excellent service delivery. Brokerages like Jefferies and Macquarie remain optimistic, citing stable credit costs and improved margins.\n",
      "\n",
      "**Market Opportunities:**\n",
      "\n",
      "1. **Leverage Growing Corporate Banking:** With banking credit expected to grow at double-digits in the coming years, HDFC Bank can capitalize on the expanding corporate banking sector. Small, medium, and large businesses are growing rapidly, presenting a significant opportunity for the bank to increase its market share.\n",
      "\n",
      "**Challenges and Risks:**\n",
      "\n",
      "While HDFC Bank has a strong position, it faces challenges such as intense competition, regulatory pressures, and the need to maintain credit quality. The bank must also navigate the evolving digital landscape and ensure robust cybersecurity measures.\n",
      "\n",
      "**ESG Strategies:**\n",
      "\n",
      "HDFC Bank is committed to Environmental, Social, and Governance (ESG) principles. The bank has set ambitious targets for sustainable finance, digital inclusion, and employee welfare, reflecting its commitment to responsible banking.\n",
      "\n",
      "**Financial KPIs:**\n",
      "\n",
      "- Gross NPA Ratio: 1.2% (down from 1.3% in the previous quarter)\n",
      "- Net NPA Ratio: 0.3% (down from 0.4% in the previous quarter)\n",
      "- Return on Assets (RoA): 1.8%\n",
      "- Return on Equity (RoE): 20.5%\n",
      "\n",
      "**Operational KPIs:**\n",
      "\n",
      "- Domestic Retail Loans: 79% of total loans\n",
      "- Domestic Retail Deposits: 84% of total deposits\n",
      "- Branch Network: 5,444 branches (as of March 2021)\n",
      "- ATMs: 16,000+\n",
      "\n",
      "**Recent Trends:**\n",
      "\n",
      "HDFC Bank has been focusing on digital transformation, with a significant emphasis on artificial intelligence, machine learning, and data analytics. The bank has also been expanding its presence in semi-urban and rural areas, aiming to cater to a broader customer base.\n",
      "\n",
      "In conclusion, HDFC Bank, with its strong market position, competitive strategies, and focus on growth opportunities, is well-poised to navigate the Indian banking landscape and capitalize on emerging trends.\n"
     ]
    }
   ],
   "source": [
    "company = \"HDFC Bank\"\n",
    "\n",
    "query = \"Create a concise 1-page market report for company \"+ company + \\\n",
    "\"that summarizes the research, competitive analysis, and highlights the market opportunities.\"\n",
    "\n",
    "try:\n",
    "    # Define the structure of the workflow graph\n",
    "    search_agent_workflow = Workflow(schema=SearchAgentState, name=\"WebSearchAgent\")\n",
    "    search_agent_workflow.add_step(\"web_search\", web_search)\n",
    "    search_agent_workflow.add_step(\"generate_answer\", generate_answer)\n",
    "\n",
    "    # Execute the workflow\n",
    "    search_response = await search_agent_workflow.run(\n",
    "        SearchAgentState(question=query)\n",
    "    )\n",
    "\n",
    "    print(\"*****\")\n",
    "    print(\"Question: \", search_response.state.question)\n",
    "    print(\"Answer: \", search_response.state.answer)\n",
    "\n",
    "except WorkflowError:\n",
    "    traceback.print_exc()\n",
    "except ValidationError:\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
